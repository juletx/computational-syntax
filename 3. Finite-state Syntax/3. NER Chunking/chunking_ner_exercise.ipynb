{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"chunking_ner_exercise.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNkz7q4Jes5ur9hMNZhJRyg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Chunking NER"],"metadata":{"id":"Qvq5POOjpOva"}},{"cell_type":"code","source":["import nltk\n","nltk.download('conll2000')\n","from nltk.corpus import conll2000"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x5RvqUhHphjP","executionInfo":{"status":"ok","timestamp":1642433624195,"user_tz":-60,"elapsed":640,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"9625a448-1e58-476b-ad92-808c7de0381e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/conll2000.zip.\n"]}]},{"cell_type":"markdown","source":["## Exercise 1: Chunking\n","\n","Let's do some data exploration.\n","1. First, how many sentences are there?\n","2. How many NP chunks?\n","3. How many VP chunks?\n","4. How many PP chunks?\n","5. What is the average length of each?"],"metadata":{"id":"r5nE6vDnpUgz"}},{"cell_type":"code","source":["sent_99 = conll2000.chunked_sents('train.txt')[99]\n","print(sent_99)\n","print(sent_99.flatten())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F2cVgF-DrSOi","executionInfo":{"status":"ok","timestamp":1642434227543,"user_tz":-60,"elapsed":252,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"c3808a8f-3965-4e8a-fa3b-e01d27e4243e"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (PP Over/IN)\n","  (NP a/DT cup/NN)\n","  (PP of/IN)\n","  (NP coffee/NN)\n","  ,/,\n","  (NP Mr./NNP Stone/NNP)\n","  (VP told/VBD)\n","  (NP his/PRP$ story/NN)\n","  ./.)\n","(S\n","  Over/IN\n","  a/DT\n","  cup/NN\n","  of/IN\n","  coffee/NN\n","  ,/,\n","  Mr./NNP\n","  Stone/NNP\n","  told/VBD\n","  his/PRP$\n","  story/NN\n","  ./.)\n"]}]},{"cell_type":"code","source":["sents = conll2000.chunked_sents('train.txt')\n","np_count, vp_count, pp_count = 0, 0, 0\n","np_len, vp_len, pp_len = 0, 0, 0\n","for sent in sents:\n","    for subtree in sent.subtrees():\n","        label = subtree.label()\n","        if label == \"NP\":\n","            np_count += 1\n","            np_len += len(subtree)\n","        if label == \"VP\":\n","            vp_count += 1\n","            vp_len += len(subtree)\n","        if label == \"PP\":\n","            pp_count += 1\n","            pp_len += len(subtree)\n","np_avg = round(np_len / np_count, 2)\n","vp_avg = round(vp_len / vp_count, 2)\n","pp_avg = round(pp_len / pp_count, 2)\n","print(\"Sentences:\", len(sents))\n","print(\"NP Chunks:\", np_count)\n","print(\"VP Chunks:\", vp_count)\n","print(\"PP Chunks:\", pp_count)\n","print(\"NP Chunks Avg Length:\", np_avg)\n","print(\"VP Chunks Avg Length:\", vp_avg)\n","print(\"PP Chunks Avg Length:\", pp_avg)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4lmzVKShpm3m","executionInfo":{"status":"ok","timestamp":1642435182813,"user_tz":-60,"elapsed":1420,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"78e7f231-0a63-4dbd-c8f6-81b2ec90caf2"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentences: 8936\n","NP Chunks: 55081\n","VP Chunks: 21467\n","PP Chunks: 21281\n","NP Chunks Avg Length: 2.15\n","VP Chunks Avg Length: 1.56\n","PP Chunks Avg Length: 1.01\n"]}]},{"cell_type":"code","source":["from nltk import FreqDist\n","from collections import defaultdict\n","import numpy as np\n","labels = FreqDist()\n","lengths = defaultdict(lambda: list())\n","\n","sents = conll2000.chunked_sents('train.txt')\n","for sent in sents:\n","    for subtree in sent.subtrees():\n","        label = subtree.label()\n","        labels.update([label])\n","        lengths[label].append(len(subtree))\n","\n","print(\"Sentences:\", len(sents))\n","for label in [\"NP\", \"VP\", \"PP\"]:\n","    print(label)\n","    print(\"Chunks:\", labels[label])\n","    print(\"Length:\", round(np.mean(lengths[label]), 2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I45pH-K5wb56","executionInfo":{"status":"ok","timestamp":1642436120465,"user_tz":-60,"elapsed":1789,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"a8f99ece-2a77-456b-c6bf-3ef60be0eca0"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentences: 8936\n","NP\n","Chunks: 55081\n","Length: 2.15\n","VP\n","Chunks: 21467\n","Length: 1.56\n","PP\n","Chunks: 21281\n","Length: 1.01\n"]}]},{"cell_type":"markdown","source":["## Exercise 2: Unigram chunker\n","\n","Now, let's concentrate only on NP chunking\n","1. Create a unigram chunker using the UnigramChunker class below.\n","Train on the train sentences and evaluate on the test sentences using\n","the evaluate method, i.e., my_model.evaluate(test_sents).\n","\n","2. What is the F1 score?"],"metadata":{"id":"gQjVdH_pptr9"}},{"cell_type":"code","source":["class UnigramChunker(nltk.ChunkParserI):\n","    def __init__(self, train_sents):\n","        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n","                      for sent in train_sents]\n","        self.tagger = nltk.UnigramTagger(train_data)\n","\n","    def parse(self, sentence):\n","        pos_tags = [pos for (word,pos) in sentence]\n","        tagged_pos_tags = self.tagger.tag(pos_tags)\n","        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n","        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n","                     in zip(sentence, chunktags)]\n","        return nltk.chunk.conlltags2tree(conlltags)\n","\n","\n","train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n","test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n","\n","unigram_chunker = UnigramChunker(train_sents)\n","print(unigram_chunker.evaluate(test_sents))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tKwe-3hwp1_t","executionInfo":{"status":"ok","timestamp":1642434857099,"user_tz":-60,"elapsed":2677,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"2e80086b-d59e-4eb6-aa26-70fd13d43dd0"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["ChunkParse score:\n","    IOB Accuracy:  92.9%%\n","    Precision:     79.9%%\n","    Recall:        86.8%%\n","    F-Measure:     83.2%%\n"]}]},{"cell_type":"markdown","source":["## Exercise 3: Bigram/Trigram chunker\n","\n","Now, modify the code to create Bigram and Trigram taggers"],"metadata":{"id":"DmqFVTczp42L"}},{"cell_type":"code","source":["class BigramChunker(nltk.ChunkParserI):\n","    def __init__(self, train_sents):\n","        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n","                      for sent in train_sents]\n","        self.tagger = nltk.BigramTagger(train_data)\n","\n","    def parse(self, sentence):\n","        pos_tags = [pos for (word,pos) in sentence]\n","        tagged_pos_tags = self.tagger.tag(pos_tags)\n","        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n","        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n","                     in zip(sentence, chunktags)]\n","        return nltk.chunk.conlltags2tree(conlltags)\n","        \n","bigram_chunker = BigramChunker(train_sents)\n","print(bigram_chunker.evaluate(test_sents))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZDCbo67Zv_sb","executionInfo":{"status":"ok","timestamp":1642434993478,"user_tz":-60,"elapsed":3215,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"b5756580-4e1d-4e4f-94c0-6a80114cb953"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["ChunkParse score:\n","    IOB Accuracy:  93.3%%\n","    Precision:     82.3%%\n","    Recall:        86.8%%\n","    F-Measure:     84.5%%\n"]}]},{"cell_type":"code","source":["class TrigramChunker(nltk.ChunkParserI):\n","    def __init__(self, train_sents):\n","        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n","                      for sent in train_sents]\n","        self.tagger = nltk.TrigramTagger(train_data)\n","\n","    def parse(self, sentence):\n","        pos_tags = [pos for (word,pos) in sentence]\n","        tagged_pos_tags = self.tagger.tag(pos_tags)\n","        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n","        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n","                     in zip(sentence, chunktags)]\n","        return nltk.chunk.conlltags2tree(conlltags)\n","        \n","trigram_chunker = TrigramChunker(train_sents)\n","print(trigram_chunker.evaluate(test_sents))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"roHd4PxayNvi","executionInfo":{"status":"ok","timestamp":1642435529099,"user_tz":-60,"elapsed":3037,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"4213e424-1749-4d48-aff8-065a255ae291"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["ChunkParse score:\n","    IOB Accuracy:  93.3%%\n","    Precision:     82.5%%\n","    Recall:        86.8%%\n","    F-Measure:     84.6%%\n"]}]},{"cell_type":"markdown","source":["## Exercise 4: Maximum Entropy model with features\n","\n","Finally, we will use a maximum entropy classifier (a discriminative classifier)\n","to model the chunking task. Remember that discriminative classifiers attempt to\n","model p(y|x) directly, which allows us more freedom in what x is."],"metadata":{"id":"q7gb3lRwp863"}},{"cell_type":"code","source":["def npchunk_features(sentence, i, history):\n","    word, pos = sentence[i]\n","    return {\"pos\": pos}\n","\n","\n","class ConsecutiveNPChunkTagger(nltk.TaggerI):\n","\n","    def __init__(self, train_sents):\n","        train_set = []\n","        for tagged_sent in train_sents:\n","            untagged_sent = nltk.tag.untag(tagged_sent)\n","            history = []\n","            for i, (word, tag) in enumerate(tagged_sent):\n","                featureset = npchunk_features(untagged_sent, i, history)\n","                train_set.append((featureset, tag))\n","                history.append(tag)\n","        self.classifier = nltk.MaxentClassifier.train(\n","            train_set, max_iter=10, trace=0)\n","\n","    def tag(self, sentence):\n","        history = []\n","        for i, word in enumerate(sentence):\n","            featureset = npchunk_features(sentence, i, history)\n","            tag = self.classifier.classify(featureset)\n","            history.append(tag)\n","        return zip(sentence, history)\n","\n","\n","class ConsecutiveNPChunker(nltk.ChunkParserI):\n","    def __init__(self, train_sents):\n","        tagged_sents = [[((w, t), c) for (w, t, c) in\n","                         nltk.chunk.tree2conlltags(sent)]\n","                        for sent in train_sents]\n","        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n","\n","    def parse(self, sentence):\n","        tagged_sents = self.tagger.tag(sentence)\n","        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n","        return nltk.chunk.conlltags2tree(conlltags)"],"metadata":{"id":"C6GvcXoxqEmg","executionInfo":{"status":"ok","timestamp":1642435941697,"user_tz":-60,"elapsed":281,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["consecutive_chunker = ConsecutiveNPChunker(train_sents)\n","print(consecutive_chunker.evaluate(test_sents))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DvZXdpgmzuKy","executionInfo":{"status":"ok","timestamp":1642436029800,"user_tz":-60,"elapsed":86343,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"3f70bab6-93f1-452a-90fa-b5bdfe4bc39b"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["ChunkParse score:\n","    IOB Accuracy:  92.9%%\n","    Precision:     79.9%%\n","    Recall:        86.8%%\n","    F-Measure:     83.2%%\n"]}]},{"cell_type":"markdown","source":["## Exercise 5: Add more features to get better performance\n","\n","Add in more features to get better performance"],"metadata":{"id":"AnoBEARqqIM5"}},{"cell_type":"code","source":["def npchunk_features(sentence, i, history):\n","    word, pos = sentence[i]\n","    if i == 0:\n","        prevword, prevpos = \"<START>\", \"<START>\"\n","    else:\n","        prevword, prevpos = sentence[i-1]\n","    if i == len(sentence)-1:\n","        nextword, nextpos = \"<END>\", \"<END>\"\n","    else:\n","        nextword, nextpos = sentence[i+1]\n","    features = {\n","        \"pos\": pos,\n","        \"word\": word,\n","        \"prevpos\": prevpos,\n","        \"prevword\": prevword,\n","        \"nextpos\": nextpos,\n","        \"nextword\": nextword,\n","    }\n","    return features"],"metadata":{"id":"P5ioT7K401yG","executionInfo":{"status":"ok","timestamp":1642436635305,"user_tz":-60,"elapsed":272,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["consecutive_chunker = ConsecutiveNPChunker(train_sents)\n","print(consecutive_chunker.evaluate(test_sents))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QpnBIioD0z5s","executionInfo":{"status":"ok","timestamp":1642436873252,"user_tz":-60,"elapsed":235208,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"157b13ea-d63d-4412-d218-e2373de9d771"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["ChunkParse score:\n","    IOB Accuracy:  95.6%%\n","    Precision:     87.8%%\n","    Recall:        91.0%%\n","    F-Measure:     89.4%%\n"]}]},{"cell_type":"markdown","source":["## Exercise 6: NER Data\n","\n","We will use a dataset of tweets annotated for Named Entities.\n","\n","```\n","@inproceedings{derczynski-etal-2016-broad,\n","    title = \"Broad {T}witter Corpus: A Diverse Named Entity Recognition Resource\",\n","    author = \"Derczynski, Leon  and\n","      Bontcheva, Kalina  and\n","      Roberts, Ian\",\n","    booktitle = \"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers\",\n","    month = dec,\n","    year = \"2016\",\n","    address = \"Osaka, Japan\",\n","    publisher = \"The COLING 2016 Organizing Committee\",\n","    url = \"https://aclanthology.org/C16-1111\",\n","    pages = \"1169--1179\",\n","    abstract = \"One of the main obstacles, hampering method development and comparative evaluation of named entity recognition in social media, is the lack of a sizeable, diverse, high quality annotated corpus, analogous to the CoNLL{'}2003 news dataset. For instance, the biggest Ritter tweet corpus is only 45,000 tokens {--} a mere 15{\\%} the size of CoNLL{'}2003. Another major shortcoming is the lack of temporal, geographic, and author diversity. This paper introduces the Broad Twitter Corpus (BTC), which is not only significantly bigger, but sampled across different regions, temporal periods, and types of Twitter users. The gold-standard named entity annotations are made by a combination of NLP experts and crowd workers, which enables us to harness crowd recall while maintaining high quality. We also measure the entity drift observed in our dataset (i.e. how entity representation varies over time), and compare to newswire. The corpus is released openly, including source text and intermediate annotations.\",\n","}\n","```"],"metadata":{"id":"_BckO2y0qOiE"}},{"cell_type":"code","source":["# load the data (1000 annotated tweets)\n","import json\n","from sklearn.metrics import f1_score\n","\n","ner_data = []\n","for line in open(\"twitter_NER.json\"):\n","    ner_data.append(json.loads(line))\n","\n","\n","# each example in ner_data is a json dictionary that contains two values we are interested in: tokens, entities\n","\n","print(ner_data[6][\"tokens\"])\n","print(ner_data[6][\"entities\"])\n","\n","# we need the training data as a list of lists, where the inner list contains tuples of (token, label) i.e., [[(token_1, label_1 ), (token_2, label_2), ...]]\n","\n","# Test data should be a list of lists with the inner list having tokens\n","\n","# Test labels should be a flat list of labels ['O', 'B-PER', 'I-PER', 'O'...]\n","\n","ner_train_data = []\n","ner_test_data = []\n","ner_test_labels = []\n","for s in ner_data[:800]:\n","    ner_train_data.append(list(zip(s[\"tokens\"], s[\"entities\"])))\n","for s in ner_data[800:]:\n","    ner_test_data.append(s[\"tokens\"])\n","    ner_test_labels.extend(s[\"entities\"])"],"metadata":{"id":"ReteVJypqiWY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 7: Hidden Markov Model"],"metadata":{"id":"ptEbKox6qsbu"}},{"cell_type":"code","source":["tagger = nltk.HiddenMarkovModelTagger.train(ner_train_data)\n","\n","\n","# Evaluate the model using the f1_score function from sklearn.metrics\n","# You should use macro F1 and make sure NOT TO COUNT the 'O' label\n","\n","f1 = 0.00\n","print(\"F1 score: {0:.3f}\".format(f1))"],"metadata":{"id":"l4xL0JQxqxm7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 7: Optional further exercises\n","\n","The previous F1 is calculated at token level. However, for NER, we often calculate F1 at entity level.\n","\n","Implement your own code to evaluate entity-level NER\n","\n","1. You will need to calculate:\n","    Precision = (number of correctly predicted entities) / (number of predicted entities)\n","\n","    Recall = (number of correctly predicted entites) / (number of gold entities)\n","\n","    F1 = (2 * Precision * Recall) / (Precision + Recall)\n","\n","\n","2. You might want to implement a helper function which, given labels in IOB2 format return a list of all the entities.\n"],"metadata":{"id":"au6cHF36qzgy"}}]}