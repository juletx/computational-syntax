{"cells":[{"cell_type":"markdown","metadata":{"id":"HRAsKOq2pWVm"},"source":["**Exercise 1: Train a simple trigram language model**\n","---\n","\n","-----\n","\n","In the first exercise, weÂ´ll save the counts directly in a dictionary\n"," which defaults to the smoothing factor (_note that this is not true smoothing\n"," as it does not account for the denominator and therefore does not create a\n"," true probability distribution, but it is enough to get started_)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"zrLwvSxbpWV0","executionInfo":{"status":"ok","timestamp":1642257143477,"user_tz":-60,"elapsed":1847,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"outputs":[],"source":["import nltk\n","from collections import defaultdict\n","import numpy as np\n","import nltk.corpus\n","from nltk.corpus import brown\n","\n","# choose a small smoothing factor\n","smoothing_factor = 0.001\n","counts = defaultdict(lambda: defaultdict(lambda: smoothing_factor))"]},{"cell_type":"markdown","metadata":{"id":"M_4aB4YmpWV4"},"source":["We'll also define two helper functions, one to get the log probability of\n","a single trigram and the second to get the log probability of a full sentence"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"wkuJMGh0pWV6","executionInfo":{"status":"ok","timestamp":1642257150480,"user_tz":-60,"elapsed":597,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"outputs":[],"source":["def logP(u, v, w):\n","    \"\"\"\n","    Compute the log probability of a specific trigram\n","    \"\"\"\n","    return np.log(counts[(u, v)][w]) - np.log(sum(counts[(u, v)].values()))\n","\n","\n","def sentence_logP(S):\n","    \"\"\"\n","    Adds the special tokens to the beginning and end.\n","    Then calculates the sum of log probabilities of\n","    all trigrams in the sentence.\n","    \"\"\"\n","    tokens = ['*', '*'] + S + ['STOP']\n","    return sum([logP(u, v, w) for u, v, w in nltk.ngrams(tokens, 3)])"]},{"cell_type":"markdown","metadata":{"id":"8mIfdrLUpWV7"},"source":["We then choose the corpus. We'll use the preprocessed Brown corpus (nltk.corpus.brown), which contains many domains.\n","To see the domains, you can run brown.categories(). We also split this into train, dev, and test sets, which we will use throughout."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AVKyLdzXpWV8","executionInfo":{"status":"ok","timestamp":1642257966975,"user_tz":-60,"elapsed":304,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"5d3c0ccf-2910-43df-d92e-c37be90a820f"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n","['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"]}],"source":["nltk.download('brown')\n","print(brown.categories())\n","sentences = brown.sents(categories='news')\n","dev_idx = int(len(sentences) * .7)\n","test_idx = int(len(sentences) * .8)\n","train = sentences[:dev_idx]\n","dev = sentences[dev_idx:test_idx]\n","test = sentences[test_idx:]"]},{"cell_type":"markdown","metadata":{"id":"J02uvHKfpWV9"},"source":["Finally, we'll collect the counts in the dictionary we set up before."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"WLQzNw_1pWV-","executionInfo":{"status":"ok","timestamp":1642257175236,"user_tz":-60,"elapsed":598,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"outputs":[],"source":["for sentence in train:\n","    # add the special tokens to the sentences\n","    tokens = ['*', '*'] + sentence + ['STOP']\n","    for u, v, w in nltk.ngrams(tokens, 3):\n","        # update the counts\n","        counts[(u, v)][w] += 1"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TwGTyERbpWWA","executionInfo":{"status":"ok","timestamp":1642257177664,"user_tz":-60,"elapsed":7,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"7a5353f0-0913-4677-80ec-51b43ef8217e"},"outputs":[{"output_type":"stream","name":"stdout","text":["-42.70140049292593\n"]}],"source":["# Now that we have the model we can use it\n","print(sentence_logP(\"what is the best sentence ?\".split()))"]},{"cell_type":"markdown","metadata":{"id":"l9sRb-QwpWWD"},"source":["**Exercise 2: (3-5 minutes)**\n","---\n","\n","-----\n","\n","**Try and find the sentence (len > 10 tokens) with the highest probability**\n","\n","1. What is the sentence with the highest probability you could find?\n","2. What is it's log probability?"]},{"cell_type":"code","source":["max = - float(\"inf\")\n","sent = []\n","for sentence in sentences:\n","    p = sentence_logP(sentence)\n","    if len(sentence) > 10 and p > max:\n","        max = p\n","        sent = sentence\n","\n","print(\" \".join(sent), max)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sZ_A_N_UsGs-","executionInfo":{"status":"ok","timestamp":1642257209073,"user_tz":-60,"elapsed":1392,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"2ddad1ee-b323-40b6-d397-89786281f873"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Heating is by individual gas-fired , forced warm air systems . -8.081395957153678\n"]}]},{"cell_type":"markdown","metadata":{"id":"vQhsfeMwpWWE"},"source":["**Exercise 3: Function for trigram model, define perplexity, find the best train domain (15-20 minutes)**\n","---\n","\n","-----"]},{"cell_type":"markdown","metadata":{"id":"WKX4m54npWWG"},"source":["First, you'll need to define a function to train the trigram models. It should return the same kind of counts dictionary as in Exercise 1."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"jcrTdMKQpWWH","executionInfo":{"status":"ok","timestamp":1642257265413,"user_tz":-60,"elapsed":360,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"outputs":[],"source":["def estimate_lm(corpus, smoothing_factor=0.001):\n","    \"\"\"This function takes a corpus and returns a trigram model (counts) trained on the corpus \"\"\"\n","    counts = defaultdict(lambda: defaultdict(lambda: smoothing_factor))\n","    for sentence in corpus:\n","        # add the special tokens to the sentences\n","        tokens = ['*', '*'] + sentence + ['STOP']\n","        for u, v, w in nltk.ngrams(tokens, 3):\n","            # update the counts\n","            counts[(u, v)][w] += 1\n","    return counts"]},{"cell_type":"markdown","metadata":{"id":"B-Zdtxo_pWWJ"},"source":["Now, you'll need to define a function to measure perplexity, which is defined as the exp(total negative log likelihood / total_number_of_tokens). See https://web.stanford.edu/~jurafsky/slp3/3.pdf for more info.\n","\n","Luckily, we already have a function to get the log likelihood of a sentence (sentence_logP). So we can iterate over the sentences in a corpus, summing the log probability of each sentence, and keeping track of the total number of tokens. Finally, you can get the NEGATIVE log likelihood and average this, finally using np.exp to exponentiate the previous result."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"7FKyk7DIpWWK","executionInfo":{"status":"ok","timestamp":1642257310992,"user_tz":-60,"elapsed":310,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"outputs":[],"source":["def perplexity(corpus):\n","    \"\"\"\n","    Perplexity is defined as the exponentiated average negative log-likelihood of a sequence. \n","    \"\"\"\n","    total_log_likelihood = 0\n","    total_token_count = 0\n","    \n","    for sentence in corpus:\n","        total_log_likelihood += sentence_logP(sentence)\n","        total_token_count += len(sentence)\n","    \n","    perplexity = np.exp(- total_log_likelihood / total_token_count)\n","    return perplexity"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"WcMsQyUdpWWL","executionInfo":{"status":"ok","timestamp":1642257313635,"user_tz":-60,"elapsed":7,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"outputs":[],"source":["test_data = [[\"I'm\", 'not', 'giving', 'you', 'a', 'chance', ',', 'Bill', ',', 'but', 'availing', 'myself', 'of', 'your', 'generous', 'offer', 'of', 'assistance', '.'], ['Good', 'luck', 'to', 'you', \"''\", '.'], ['``', 'All', 'the', 'in-laws', 'have', 'got', 'to', 'have', 'their', 'day', \"''\", ',', 'Adam', 'said', ',', 'and', 'glared', 'at', 'William', 'and', 'Freddy', 'in', 'turn', '.'], ['Sweat', 'started', 'out', 'on', \"William's\", 'forehead', ',', 'whether', 'from', 'relief', 'or', 'disquietude', 'he', 'could', 'not', 'tell', '.'], ['Across', 'the', 'table', ',', 'Hamrick', 'saluted', 'him', 'jubilantly', 'with', 'an', 'encircled', 'thumb', 'and', 'forefinger', '.'], ['Nobody', 'else', 'showed', 'pleasure', '.'], ['Spike-haired', ',', 'burly', ',', 'red-faced', ',', 'decked', 'with', 'horn-rimmed', 'glasses', 'and', 'an', 'Ivy', 'League', 'suit', ',', 'Jack', 'Hamrick', 'awaited', 'William', 'at', 'the', \"officers'\", 'club', '.'], ['``', 'Hello', ',', 'boss', \"''\", ',', 'he', 'said', ',', 'and', 'grinned', '.'], ['``', 'I', 'suppose', 'I', 'can', 'never', 'expect', 'to', 'call', 'you', \"'\", 'General', \"'\", 'after', 'that', 'Washington', 'episode', \"''\", '.'], ['``', \"I'm\", 'afraid', 'not', \"''\", '.']]"]},{"cell_type":"markdown","metadata":{"id":"-Tr0fAGQpWWM"},"source":["Finally, use *estimate_lm()* to train LMs on each domain in brown.categories() and \n","find which gives the lowest perplexity on test_data. \n","\n","1. Which domain gives the best perplexity?\n","2. Can you think of a way to use language models to predict domain?"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QUUeG9kUpWWM","executionInfo":{"status":"ok","timestamp":1642257325852,"user_tz":-60,"elapsed":8855,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"d325852a-14b1-4767-c024-e44fc89caebd"},"outputs":[{"output_type":"stream","name":"stdout","text":["adventure 17.5115745216489\n","belles_lettres 23.839993737294026\n","editorial 14.207547015074889\n","fiction 19.59697351759496\n","government 11.068992137052321\n","hobbies 14.647528814565485\n","humor 8.230644148155282\n","learned 18.389202856560622\n","lore 24.26379000002405\n","mystery 17.93675812239107\n","news 18.720360984322056\n","religion 13.543580787870582\n","reviews 8.790179135397917\n","romance 3.2907736822940756\n","science_fiction 10.460016796365833\n","Domain with lowest perplexity: romance 3.2907736822940756\n"]}],"source":["min = float(\"inf\")\n","dom = \"\"\n","for domain in brown.categories():\n","    train = brown.sents(categories=domain)\n","    counts = estimate_lm(train)\n","    perp = perplexity(test_data)\n","    print(domain, perp)\n","    if perp < min:\n","        min = perp\n","        dom = domain\n","\n","print(\"Domain with lowest perplexity:\", dom, min)"]},{"cell_type":"markdown","metadata":{"id":"HdvKsJu7pWWN"},"source":["**Exercise 4: Generation**\n","---\n","\n","-----"]},{"cell_type":"markdown","metadata":{"id":"KE0OKJDrpWWN"},"source":["For the next exercise, you will need to generate 10 sentences for each domain in the Brown corpus. The first thing we need is code to be able to sample the next word in a trigram. We'll do this by creating a probability distribution over the values in our trigram counts. Remember that each key in the dictionary is a tuple (u, v) and that the values is another dictionary with the count of the continuation w: count. Therefore, we can create a numpy array with the continuation values and divide by the sum of values to get a distribution. Finally, we can use np.random.multinomial to sample from this distribution."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"C0ELtYENpWWO","executionInfo":{"status":"ok","timestamp":1642257696901,"user_tz":-60,"elapsed":292,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"outputs":[],"source":["def sample_next_word(u, v):\n","    keys, values = zip(* counts[(u, v)]. items())\n","    # convert values to np.array\n","    values = np.array(values)\n","    # divide by sum to create prob. distribution\n","    values /= values.sum()  \n","    # return the key (continuation token) for the sample with the highest probability\n","    return keys[np.argmax(np.random.multinomial(1, values))]  "]},{"cell_type":"markdown","metadata":{"id":"tsE0jYDnpWWP"},"source":["Now we can create a function that will generate text using our trigram model. You will need to start out with the two special tokens we used to train the model, and continue adding to this output, sampling the next word at each timestep. If the word sampled is the end token ('STOP'), then stop the generation and return the sequence as a string."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"-1kcqLr2pWWP","executionInfo":{"status":"ok","timestamp":1642257592065,"user_tz":-60,"elapsed":309,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"outputs":[],"source":["def generate():\n","    \"\"\"\n","    Sequentially generates text using sample_next_word().\n","    When the token generated is 'STOP', it returns the generated tokens as a string,\n","    removing the start and end special tokens.\n","    \"\"\"\n","    result = ['*', '*']\n","    i = 0\n","    word = \"\"\n","    while word != \"STOP\":\n","        word = sample_next_word(result[i], result[i + 1])\n","        result.append(word)\n","        i += 1\n","\n","    clean = result[2:len(result)-1]\n","    return \" \".join(clean)"]},{"cell_type":"markdown","metadata":{"id":"5l9ALr1zpWWQ"},"source":["Finally, use the code above to generate 10 sentences per domain in the Brown corpus.\n","\n","1. Do you see any correlation between perplexity scores and generated text?"]},{"cell_type":"code","source":["for domain in brown.categories():\n","    train = brown.sents(categories=domain)\n","    counts = estimate_lm(train)\n","    print(\"DOMAIN:\", domain)\n","    sentences = []\n","    for _ in range(10):\n","        sentence = generate()\n","        sentences.append(sentence.split())\n","        print(sentence)\n","    perp = perplexity(sentences)\n","    print(\"PERPLEXITY:\", perp)\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nz6IecBL1fGn","executionInfo":{"status":"ok","timestamp":1642257706960,"user_tz":-60,"elapsed":8619,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"a9447fa7-3e4c-4b3f-f4d3-cd258aee9718"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["DOMAIN: adventure\n","There was no reply so he spent most of the stockade wall just out of a politician as well enjoy it .\n","A tight wagon meant so much to give them privacy on their way back there when you leave .\n","The girl asked .\n","They appeared to disapprove of my background applying at the hall , could only play the last place of any customers walking in at the doctor from the outlaw , kept his face .\n","The bullet had torn through the rain and across the brushy swells told him .\n","Morgan laughed .\n","If he wondered whether the attackers would allow him to pull away unmolested , he doesn't deserve to lie there in this country within 3 days , your time will come along '' , said Tilghman .\n","He drew a long time that had frightened her .\n","`` I mean '' --\n","He supervised the cleanups and handled the shipments of raw gold which each week went out to his lack of fitness for courting .\n","PERPLEXITY: 3.237558726414217\n","\n","DOMAIN: belles_lettres\n","In this way and that , unlike the novelists and poets who had returned to Savannah .\n","The plan is admirably fulfilled for the first time , impair our own economic system , in a war veteran with battle-shattered knees .\n","Is this then the frontier is because he is compelled to find `` a scrivener within Temple Bar , that is still the right to secede be put .\n","`` Transparent look , waxy skin -- could well be almost a year in lands and stocks and bonds .\n","If living Jews were unavailable for study , we have examined , there was no Union cavalry east of the communication is between nearest neighbors , h and e have six , with Franklin K. Lane , ended on a sea of wonder .\n","A letter signed `` Isabell Bardall '' entreated `` Good Cozen '' Quiney to find ways to the loss of tradition .\n","Steele first answers briefly the charges , and a few moments and then scale each planetary deferent larger and more in Chapel .\n","The narrator is an event or series of hoops , the cosmological , philosophical , but of the room .\n","To meet situations of prolonged labor-management stalemate ; ;\n","Reverend Cotton preached to them .\n","PERPLEXITY: 3.6419435884119853\n","\n","DOMAIN: editorial\n","Broadway\n","If we want it or not , one finds that there is a lack of purchasing power throughout outstate Missouri and Southern Democrats join to defeat the school bills , the Western Hemisphere from northern Canada to Cape May and is small enough to last two weeks earlier than usual .\n","But there is no criticism of this state exist to aid the people of Western Europe are overwhelmingly opposed to participation in a two-disc set , interpreted by Lincoln authority and lecturer Roy P. Basler .\n","It follows that teachers as a ballast .\n","At the head of the asters by the French , have a terrific uplift .\n","A nuclear pacifier of these dimensions -- roughly some six and a new patient .\n","The fewer nos she has to say something about a dozen men a day .\n","But for the people .\n","We did not then , and , through the Declaration of Independence , that Sophocles would one day a week according to Mr. Khrushchev the idea of a bad one .\n","It might be willing to forgive others before we learned what it should be good examples of American States broke diplomatic relations with him to attend the Godkin Lectures is marred throughout by too forceful a desire to get gulled .\n","PERPLEXITY: 2.951268803233285\n","\n","DOMAIN: fiction\n","`` Do you think he could to calm the displeased bird .\n","The girlish voice was ripe and full and calm .\n","He felt good .\n","In the starlight he could see .\n","This was one of the slow-moving British column , but that the soul slept after death .\n","`` I can't '' .\n","These were the bedrooms .\n","He smiled .\n","They buy some supplies from our colored grocers and they never saw him heaving it again , her eyes calm .\n","My camp-made leather wallet , bulky with twisted , raised stitches around the house and the sum of her having been so close to hear .\n","PERPLEXITY: 3.761030427453342\n","\n","DOMAIN: government\n","And yet , despite the inadequacy of the Army , Navy , and for present and future water needs , the Commissioner of Internal Revenue as to the agency both broad responsibilities , touching on almost all aspects of this report , 37,470 new cases were entered into in a letter .\n","( B )\n","These were selected carefully and included not only for this purpose .\n","Yet an economy which is not a flow ! !\n","It is not the recommendation of the Administrative Procedure Act of September 28 , 1951 ( 65 Stat. 365 ) .\n","Matching requirements\n","The work will cover the most urgent priorities for their immediate attention , and added machine tool capacity .\n","Fashion centers are now operational , the administrative process .\n","We have entered into a new Congressman .\n","To encourage exploration for domestic sources of information basic to administrative decisions , a new Congressman .\n","PERPLEXITY: 3.4845996549946623\n","\n","DOMAIN: hobbies\n","Mr. Barcus also wished all the time , perhaps weeks , continuing their correspondence after he left everything to his own plants .\n","Their seeds produce vigorous blooming plants half again the size of the dirt and weather exposure and cannot enter the driveway that leads to the Finals for the submarine '' .\n","On that date the Musicians Emergency Fund , organized to furnish employment for musicians unable to obtain a permanent record .\n","About that time he was harnessed he stood like a large pear , but away from the great oaks fed the small Fiat 500 and to the fantastic profusion of wild flowers flourishing in these fields is imagination , his native talents were predisposed .\n","From the town building inspector was paying a friendly , if you haven't made avocado a part of the economy , we have an aircraft to gather that post-attack reconnaissance .\n","Fair Sail ( Farvel-Topsy Herring ) 2:36 ; ;\n","This year several entries from as far west as Wisconsin and as the Dallas Fair Park with its historic buildings and churches .\n","Study surface of clay for sides each about Af ( long enough and wide enough for you , as the American hunter are the possibilities for operating your cafeteria location to make the area is vigorously exercised in alternate sectors by alternate exercises so the pulleys won't slip when pulled by hand ; ;\n","Little folks set :\n","Remove lid from head of mold ) .\n","PERPLEXITY: 2.5256834106049144\n","\n","DOMAIN: humor\n","He went down to three short speeches ? ?\n","A biting , pithy parable of the sugar bowl on the day .\n","`` Dearly beloved '' , cried Mrs. Crombie was standing in his `` lines '' , the curtain rises on a field trip in some mysterious way , they did not .\n","Could such unadulterated love , for existence , a sitting room , dressing rooms for everybody , even going so far as I can lead you to an end ' ! !\n","As we find out -- and he had attained a share of them said .\n","Not only was Haumd's intonation and phrasing without flaw , but , oddly enough , this happened every four or five months .\n","The night he saw it , and had taken us in comedy and satire '' .\n","people who read Daphne du Maurier , and though at times a bit obscure , the children maneuvered out of the rights of a certain `` residential club for young actresses '' , the response does not believe in the dress department and was transferred to the Bronx Zoo to grimace at the end of the brothel , the Minute Women , the Quizzical Salamander .\n","`` I personally enjoy your newspaper as much as my husband and leading man , Letch Feeley and the terrible strain he imposed on me to do with General Burnside was going to do with a dozen similar instances , and he didn't like to work at the new grandeur of France and myself '' into every role and `` helpmate '' had never learned his part .\n","Not only were the color of tallow , was a trifle too voluptuous .\n","PERPLEXITY: 2.4481278240835924\n","\n","DOMAIN: learned\n","In our work the best of the Hellenic outlook , independent of pressure and then to Af where x is the Powell Amendment , which is included in the Philippines , and around the attitudes and opinions of small amounts of inactive chlorine with liquid air trap with only a small number of groups and individuals .\n","Why ? ?\n","Simultaneously , a mineralogical name index to organic compounds .\n","nor did Synthetic Cubism began with the cathode below .\n","The anode holder instead of seeing objects in space are bound to mean a small firm selling restaurant products , which is his privilege .\n","Chapter 10 , 8 liters of water at 100 - 109-degrees-F ( ( fig. 26 ) .\n","Each entry that is capable of .\n","The Barker index is essentially that of loose-leaf sheets ( Af ) .\n","In the international community this reduced law to Jellinek's auto-limitation .\n","The combined threat of effective anti-trust action , and for recording the contraction of smooth muscles under various external loadings .\n","PERPLEXITY: 3.974482888837638\n","\n","DOMAIN: lore\n","The artist who paints in oil uses drying oils .\n","Feeds for livestock took about one-sixth of the school in 1769 , naming it after the beginning of a stampede .\n","However , the Lambeth Conference again affirmed the primary objective purpose of this political assassination on Negroes in the gallery , were unattached , and all woodwork was treated by applying liquid bluing .\n","`` A maid told us they saw that both ladies were bleeding from scratches as they feel a certain number of her in a single position , a nineteen-year-old machinist observed irritably .\n","If asked how this statement would normally be made .\n","The farm value of all faiths to unite in an area that could not be lost .\n","The folk are simply not homogeneous with respect to a human wreck -- thanks to Webster's eloquence .\n","once the pool was a believer in the middle of the United States .\n","Outdoor exercises\n","It was a cheetah .\n","PERPLEXITY: 3.716813964058064\n","\n","DOMAIN: mystery\n","Nor would he choose a respectable hotel as the Rebel yell .\n","I asked .\n","Sea-road , railroad , lack of appreciation .\n","It was the hope it would have come out of money , enough .\n","Rev said , `` What you don't even look up something in the drizzle , trying to make notes as he could be pompous as well .\n","There had been set off to fantastic depths almost from the South last year , but later , when he came out here shivering in the bottom of the men and women struggling into their wraps .\n","He was on the table that dusky , drizzling rain touched her face .\n","`` I'm not exactly jumping up and saw with satisfaction , `` There may not want to leave .\n","Who ? ?\n","It stretched to an impossible height , climbing the wall opposite , and he chuckled .\n","PERPLEXITY: 3.8883315667539704\n","\n","DOMAIN: news\n","In People vs. Fisher , Justice Savage of the United States leadership has not been consulted yet about the American League champion New York Yankees .\n","A plea of nolo contendere , followed by 11 states .\n","Williams is a matter `` of our plans '' ! !\n","Decorating the ballroom will be called shortly by President Kennedy had been reduced greatly in size , you do something to the U.S. and foreign cars .\n","Behind this reply , Deputy Police Commissioner Howard R. Leary said that further delay in the police car to be a short one .\n","it maintains a central book collection of 100,000 nonfiction volumes as the fall of 1954 called for a Sousa march , the shrinking strike zone , and the International Association of Commerce and Industry ) expect to establish fully the thriving systems of today .\n","The student newspaper , The Queen of Comus , have been scheduled for 7 p.m. , the 15th hole , was canvassed at the drop of blood ! !\n","`` I've been looking at the White Sox and Rocky Colavito , then went beyond the required non-partisan ballot for posts on the academic community are prone to imagine that this has been elected president of the Republicans , Hughes asked , `` we know , that the verdict was against Southeast Missouri last Friday .\n","Third , there is no proof of Weinstein's entering a candidate in recent months and reportedly received anonymous telephone calls at their home in Santa Barbara , Calif. .\n","Walker said he will deliver tomorrow night to the strongest men and five years did clerical work with a plan there to Laos to verify the cease-fire by the state capitol on Tuesday .\n","PERPLEXITY: 3.2030425743003814\n","\n","DOMAIN: religion\n","What is there about us that these electrons are moving in great circles and ellipses , and permanence .\n","The normal rate of suicides in East Berlin was one of Finney's techniques as they -- only in so far from the beginning on the teachings of the ocean beyond .\n","What makes this long and diverse tradition essentially one is deliberately willed or intended and done against the inability of political and religious institutions is like proposing that they had served as the outcome of works , accept it as precisely , as members of drunkenness , fighting , malicious gossip , lying , cheating , sexual irregularities , gambling , horse racing , and does not intend that those who are not concerned with religious matters and without really very much need .\n","Meanwhile , the spirit of the community .\n","`` You must be made and that religious population shifts have emptied churches , a medical columnist , thinks it would be their first experience in membership training , including those coming by transfer of membership .\n","It means to say and the fact that many local churches they have no redeeming features whatsoever .\n","not that they are right next door .\n","The trend throughout the land .\n","Early Chinese anchoritism was theoretically aimed at more personal benefits from magical powers .\n","The choice is yours : the opposition of pure consciousness to ratiocinating consciousness .\n","PERPLEXITY: 2.41726054648677\n","\n","DOMAIN: reviews\n","It was all ears and eyes just after Act 2 , Act 1 , brought some disconcerting applause even before she had chosen to sing , or whom , he believes , `` Journal '' , a nice violence and no one gets killed .\n","But while she is entitled to something like , say , a shell among other shells .\n","The author begins this volume with a woman sang , she really does not .\n","And to an electronic score by Alwin Nikolais .\n","and the centers in which Sancho Panza , in his duty '' .\n","As faulty as has come back among us again .\n","But it's hard to find .\n","New Hampshire figures its peak , especially in the costumes of the maidens and built steadily into typical Moiseyev vigor and warmth ; ;\n","The mule is honored at Benson , N.C. ( Sept. 23 headlining the Three Stooges and Pee Wee Russell , Johnny St. Cyr , Joe Sullivan , Red Allen , Lil Armstrong , Blossom Seeley .\n","Everybody returned after intermission for the star release , Pass In Review ( SP-44001 ) .\n","PERPLEXITY: 2.4054886357071776\n","\n","DOMAIN: romance\n","`` He wasn't only different -- he did have a drink with newspaper people .\n","In Rome , Italy , not even look at him .\n","`` Surely , Captain , may I speak , please '' , he felt like a mole on a more tranquil , a guy's a guy and Lucille's willing to -- to come along while I get there .\n","`` Always '' .\n","A very great Pope , in that '' ? ?\n","`` We all feel guilty '' , he added anything to quench it .\n","The way she walked out onto Virginia Street , he said heatedly before he came home from a hot bath , and after he has people to know all about me every day ? ?\n","She had some amusing scandal about the nightmare .\n","And then came the hairpin turn , the ball came in .\n","I don't really believe in intuition .\n","PERPLEXITY: 3.6555362143414287\n","\n","DOMAIN: science_fiction\n","No doubt the Angels had of thinking of everything in terms of absolutes , that the Federation Assembly Daily Record should be translated into the cabin , they thought it exceedingly profound , though in public ceremony .\n","The good man chortled appreciatively and decided the trip what he has given '' .\n","Had he decided , perhaps , especially in an Angel , whose assumptions had mostly been fixed millions of years ago .\n","In privacy he could do but wait .\n","Ekstrohm was startled in the initial transferral but of more importance to her .\n","Self's integrity was and is and ever had been resting , sleeplessly .\n","`` I wasn't planning on jumping you .\n","The buddies invariably took advantage of him .\n","`` No problem at all .\n","A Tibetan swami from Palermo , Sicily , announced the Church's second Major Miracle : Supreme Bishop Digby had been there .\n","PERPLEXITY: 2.016848711481119\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"jo2frjAhpWWQ"},"source":["**Exercise 5: Smoothing**\n","---\n","\n","-----"]},{"cell_type":"markdown","metadata":{"id":"G-DZ0wZdpWWR"},"source":["So far, we have been using a kind of stupid smoothing technique, giving up entirely on computing an actual probability distribution. For this section, let's implement a correct version of Laplace smoothing. You'll need to keep track of the vocabulary as well, and don't forget to add the special tokens."]},{"cell_type":"code","execution_count":19,"metadata":{"id":"iFPC51-4pWWS","executionInfo":{"status":"ok","timestamp":1642258070546,"user_tz":-60,"elapsed":294,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"outputs":[],"source":["def estimate_lm_smoothed(corpus, alpha=1):\n","    counts = defaultdict(lambda: defaultdict(lambda: 0))\n","    vocab = set(['*', 'STOP'])\n","\n","    for sentence in corpus:\n","        # add the special tokens to the sentences\n","        tokens = ['*', '*'] + sentence + ['STOP']\n","        for token in sentence:\n","            vocab.add(token)\n","        for u, v, w in nltk.ngrams(tokens, 3):\n","            # update the counts\n","            counts[(u, v)][w] += 1\n","\n","    return counts, vocab"]},{"cell_type":"markdown","metadata":{"id":"T9nlFtz1pWWT"},"source":["The main change is not in how we estimate the counts, but in how we calculate log probability for each trigram.\n","Specifically, we need to add the size_of_the_vocabulary * alpha to the denominator."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"xDtNaaGcpWWU","executionInfo":{"status":"ok","timestamp":1642258437632,"user_tz":-60,"elapsed":291,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"outputs":[],"source":["def logP_smoothed(u, v, w, V, alpha=1):\n","    \"\"\"\n","    Compute the log probability of a specific trigram\n","    \"\"\"\n","    return np.log(counts[(u, v)][w] + alpha) - np.log(sum(counts[(u, v)].values()) + len(V)*alpha)\n","\n","def sentence_logP_smoothed(S, V, alpha=1):\n","    \"\"\"\n","    Adds the special tokens to the beginning and end.\n","    Then calculates the sum of log probabilities of\n","    all trigrams in the sentence using logP_smoothed.\n","    \"\"\"\n","    tokens = ['*', '*'] + S + ['STOP']\n","    return sum([logP_smoothed(u, v, w, V, alpha) for u, v, w in nltk.ngrams(tokens, 3)])\n","\n","def perplexity_smoothed(corpus, V, alpha=1):\n","    \"\"\"\n","    Perplexity is defined as the exponentiated average negative log-likelihood of a sequence. In this case, we approximate perplexity over the full corpus as an average of sentence-wise perplexity scores.\n","    \"\"\"\n","    total_log_likelihood = 0\n","    total_token_count = 0\n","    \n","    for sentence in corpus:\n","        total_log_likelihood += sentence_logP_smoothed(sentence, V, alpha)\n","        total_token_count += len(sentence)\n","    \n","    perplexity = np.exp(- total_log_likelihood / total_token_count)\n","    return perplexity"]},{"cell_type":"markdown","metadata":{"id":"uS4fsXHUpWWU"},"source":["Now train s_counts and vocab and compare perplexity with the original version on the heldout test set."]},{"cell_type":"code","source":["for domain in brown.categories():\n","    sentences = brown.sents(categories=domain)\n","    dev_idx = int(len(sentences) * .7)\n","    test_idx = int(len(sentences) * .8)\n","    train = sentences[:dev_idx]\n","    dev = sentences[dev_idx:test_idx]\n","    test = sentences[test_idx:]\n","    train = brown.sents(categories=domain)\n","    counts = estimate_lm(train)\n","    perp = perplexity(test)\n","    print(domain, perp)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2s2sBZOxO5Wm","executionInfo":{"status":"ok","timestamp":1642258507380,"user_tz":-60,"elapsed":13891,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"cb730870-49d5-491b-e9b8-6d5d8f856128"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["adventure 3.62618443256563\n","belles_lettres 3.9012572602789732\n","editorial 2.864068521672823\n","fiction 3.5531091352058013\n","government 2.928235898815161\n","hobbies 2.9136126539130376\n","humor 2.154126063174749\n","learned 4.061666793956146\n","lore 3.3842649255871065\n","mystery 3.642513970189877\n","news 3.127661915070667\n","religion 2.876022689107901\n","reviews 2.288718842401025\n","romance 3.760835096939757\n","science_fiction 2.384818394798587\n"]}]},{"cell_type":"code","source":["for domain in brown.categories():\n","    sentences = brown.sents(categories=domain)\n","    dev_idx = int(len(sentences) * .7)\n","    test_idx = int(len(sentences) * .8)\n","    train = sentences[:dev_idx]\n","    dev = sentences[dev_idx:test_idx]\n","    test = sentences[test_idx:]\n","    train = brown.sents(categories=domain)\n","    counts, vocab = estimate_lm_smoothed(train)\n","    perp = perplexity_smoothed(test, vocab)\n","    print(domain, perp)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p9aLkfeBENKv","executionInfo":{"status":"ok","timestamp":1642258466525,"user_tz":-60,"elapsed":12351,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"0dd6ad6e-cc50-46a5-d6f7-f1d7200bafa8"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["adventure 4658.570954741525\n","belles_lettres 9373.49493686855\n","editorial 5781.57447517779\n","fiction 4880.377736383577\n","government 4583.126411354653\n","hobbies 7110.193799545558\n","humor 3001.1276205544236\n","learned 8722.054703487684\n","lore 7885.851210182688\n","mystery 4014.3626080576055\n","news 8364.05170275737\n","religion 3809.1778548140337\n","reviews 5372.713225295355\n","romance 4421.488819829962\n","science_fiction 2028.2703269245585\n"]}]},{"cell_type":"markdown","source":["**Exercise 6: Find the optimal smoothing factor**\n","---\n","\n","-----"],"metadata":{"id":"n9cAgxBGPRvU"}},{"cell_type":"markdown","source":["Now create a development set using some of the training data and find the optimal smoothing factor and retest on the test set"],"metadata":{"id":"SnEksipWPvWk"}},{"cell_type":"code","source":["full_corpus = brown.sents()\n","train_idx = int(len(full_corpus) * .7)\n","dev_idx = int(len(full_corpus) * .8)\n","train = full_corpus[:train_idx]\n","dev = full_corpus[train_idx:dev_idx]\n","test = full_corpus[dev_idx:]\n","\n","best_alpha = 1000\n","best_perplex = 100000\n","\n","for alpha in [100, 10, 1, 0.1, 0.01, 0.001, 0.0001]:\n","    s_counts, vocab = estimate_lm_smoothed(train)\n","    dev_perplex = perplexity_smoothed(dev, vocab, alpha=alpha)\n","    if dev_perplex < best_perplex:\n","        best_alpha = alpha\n","        best_perplex = dev_perplex\n","        print(\"Current best: {0:.2f} with alpha={1}\".format(best_perplex,\n","                                                            best_alpha))\n","\n","print(\"Best alpha: {0}\".format(best_alpha))\n","print(\"Smoothed perplexity: {0:.3f}\".format(perplexity_smoothed(test, vocab, alpha=best_alpha)))\n","print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YbZBhj8OPq82","executionInfo":{"status":"ok","timestamp":1642258832153,"user_tz":-60,"elapsed":45150,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"17ba2469-b763-4d85-c501-d3e32946b00e"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Current best: 95829.10 with alpha=100\n","Current best: 87428.61 with alpha=10\n","Current best: 72553.08 with alpha=1\n","Current best: 55528.10 with alpha=0.1\n","Current best: 40756.69 with alpha=0.01\n","Current best: 20990.22 with alpha=0.001\n","Current best: 4709.40 with alpha=0.0001\n","Best alpha: 0.0001\n","Smoothed perplexity: 2747.252\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"yFiGZSm0pWWV"},"source":["**Exercise 7: Interpolation**\n","---\n","\n","-----"]},{"cell_type":"markdown","metadata":{"id":"nBlA1hxCpWWW"},"source":["To be able to interpolate unigram, bigram, and trigram models, we first need to train them. So here you need to make a function that takes 1) a corpus and 2) an n-gram (1,2,3) and 3) a smoothing factor and returns the counts and vocabulary. Notice that for the unigram model, you will have to set up the dictionary in a different way than we have done until now."]},{"cell_type":"code","execution_count":29,"metadata":{"id":"t3LadlgipWWX","executionInfo":{"status":"ok","timestamp":1642259259699,"user_tz":-60,"elapsed":304,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"outputs":[],"source":["def estimate_ngram(corpus, N=3, smoothing_factor=1):\n","    vocab = set(['*', 'STOP'])\n","    if N > 1:\n","        # set up the counts like before\n","        counts = defaultdict(lambda: defaultdict(lambda: 0))\n","        for sentence in corpus:\n","            # add the special tokens to the sentences\n","            vocab.update(sentence)\n","            tokens = ['*'] * (N - 1) + sentence + ['STOP']\n","            for ngram in nltk.ngrams(tokens, N):\n","                # update the counts\n","                counts[ngram[:-1]][ngram[-1]] += 1\n","    else:\n","        # set them up as necessary for the unigram model\n","        counts = defaultdict(lambda: 0)\n","        for sentence in corpus:\n","            vocab.update(sentence)\n","            for token in sentence:\n","                counts[token] += 1\n","        \n","    # Finish the code here\n","    return counts, vocab"]},{"cell_type":"markdown","metadata":{"id":"G5mVUWYMpWWX"},"source":["You will also need separate functions to get the log probability for each ngram."]},{"cell_type":"code","execution_count":30,"metadata":{"id":"7fltNH9upWWX","executionInfo":{"status":"ok","timestamp":1642259377928,"user_tz":-60,"elapsed":303,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"outputs":[],"source":["def logP_trigram(counts, u, v, w, vocab, alpha=1):\n","    \"\"\"\n","    Compute the log probability of a specific trigram\n","    \"\"\"\n","    return np.log(counts[(u, v)][w] + alpha) - np.log(sum(counts[(u, v)].values()) + len(vocab)*alpha)\n","\n","def logP_bigram(counts, u, v, vocab, alpha=1):\n","    \"\"\"\n","    Compute the log probability of a specific bigram\n","    \"\"\"\n","    return np.log(counts[u][v] + alpha) - np.log(sum(counts[u].values()) + len(vocab)*alpha)\n","\n","def logP_unigram(counts, u, vocab, alpha=1):\n","    \"\"\"\n","    Compute the log probability of a specific unigram\n","    \"\"\"\n","    return np.log(counts[u] + alpha) - np.log(sum(counts.values()) + len(vocab)*alpha)"]},{"cell_type":"markdown","metadata":{"id":"VOvGKFiApWWY"},"source":["In this case, the main change is in calculating the log probability of the sentence. "]},{"cell_type":"code","execution_count":34,"metadata":{"id":"2yI79YqXpWWY","executionInfo":{"status":"ok","timestamp":1642259745073,"user_tz":-60,"elapsed":300,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"outputs":[],"source":["def sentence_interpolated_logP(S, vocab, uni_counts, bi_counts, tri_counts, lambdas=[0.5, 0.3, 0.2], alpha=1):\n","    tokens = ['*', '*'] + S + ['STOP']\n","    prob = 0\n","    # Calculate the log probabilities for each ngram and then multiply them by the lambdas and sum them.\n","    for u, v, w in nltk.ngrams(tokens, 3):\n","        tri_prob = logP_trigram(tri_counts, u, v, w, vocab, alpha)\n","        bi_prob = logP_bigram(bi_counts, u, v, vocab, alpha)\n","        uni_prob = logP_unigram(uni_counts, u, vocab, alpha)\n","        prob += lambdas[0] * tri_prob + lambdas[1] * bi_prob + lambdas[2] * uni_prob\n","\n","    return prob\n","\n","def interpolated_perplexity(corpus, vocab, uni_counts, bi_counts, tri_counts, lambdas=[0.5, 0.3, 0.2], alpha=1):\n","    \"\"\"\n","    Perplexity is defined as the exponentiated average negative log-likelihood of a sequence. \n","    In this case, we approximate perplexity over the full corpus as an average of sentence-wise perplexity scores.\n","    \"\"\"\n","    total_log_likelihood = 0\n","    total_token_count = 0\n","    \n","    for sentence in corpus:\n","        total_log_likelihood += sentence_interpolated_logP(sentence, vocab, uni_counts, bi_counts, tri_counts, lambdas, alpha)\n","        total_token_count += len(sentence)\n","    \n","    perplexity = np.exp(- total_log_likelihood / total_token_count)\n","    return perplexity"]},{"cell_type":"markdown","metadata":{"id":"HWNquFK_pWWa"},"source":["Finally, train unigram, bigram, and trigram models and compute the perplexity of the interpolated model on the test set."]},{"cell_type":"code","source":["for domain in brown.categories():\n","    sentences = brown.sents(categories=domain)\n","    dev_idx = int(len(sentences) * .7)\n","    test_idx = int(len(sentences) * .8)\n","    train = sentences[:dev_idx]\n","    dev = sentences[dev_idx:test_idx]\n","    test = sentences[test_idx:]\n","    train = brown.sents(categories=domain)\n","    uni_counts, vocab = estimate_ngram(train, N=1)\n","    bi_counts, vocab = estimate_ngram(train, N=2)\n","    tri_counts, vocab = estimate_ngram(train, N=3)\n","    perp = interpolated_perplexity(test, vocab, uni_counts, bi_counts, tri_counts, alpha=best_alpha)\n","    print(domain, perp)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9r2AhNspTwCh","executionInfo":{"status":"ok","timestamp":1642259816612,"user_tz":-60,"elapsed":51321,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"a83b995f-b13a-48e1-81e6-c53dd115183b"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["adventure 250.96455308415733\n","belles_lettres 304.05248614516256\n","editorial 208.6668680699024\n","fiction 227.41646083281498\n","government 185.91414516263308\n","hobbies 245.40898649261095\n","humor 122.90472507564117\n","learned 331.29422014674515\n","lore 258.66422440150126\n","mystery 233.26343190817988\n","news 271.2335662862673\n","religion 188.12725095094245\n","reviews 183.30263047121545\n","romance 244.66821364231544\n","science_fiction 122.09443100947084\n"]}]},{"cell_type":"markdown","metadata":{"id":"iryUJsKHpWWa"},"source":["**Exercise 8: Build a simple spelling corrector**\n","---\n","\n","-----"]},{"cell_type":"markdown","metadata":{"id":"eB9z9brQpWWb"},"source":["In this section, we will build a simple spelling corrector with two components: 1) a dictionary of common spelling errors which will allow us to create possible hypothesis sentences and 2) a language model to filter the most likely sentence."]},{"cell_type":"code","execution_count":40,"metadata":{"id":"ZseyRCGNpWWb","executionInfo":{"status":"ok","timestamp":1642260013321,"user_tz":-60,"elapsed":265,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"outputs":[],"source":["common_errors = {\"ei\": \"ie\",  # acheive: achieve\n","                 \"ie\": \"ei\",  # recieve: receive\n","                 \"ant\": \"ent\",  # apparant: apparent\n","                 \"m\": \"mm\",  # accomodate: accommodate\n","                 \"s\": \"ss\",  # profesional: professional\n","                 \"teh\": \"the\",\n","                 \"too\": \"to\",\n","                 \"their\": \"there\",\n","                 \"there\": \"they're\"\n","                 }"]},{"cell_type":"code","execution_count":59,"metadata":{"id":"rCSg9EyApWWc","executionInfo":{"status":"ok","timestamp":1642260552461,"user_tz":-60,"elapsed":259,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"outputs":[],"source":["test_set = [\"I do not know who acheived it\".split(),\n","            \"I do not know who recieved it\".split(),\n","            \"That is not apparant\".split(),\n","            \"We can only accomodate one person\".split(),\n","            \"That is not profesional\".split(),\n","            \"we saw teh man running\".split(),\n","            \"We tried too help them\".split(),\n","            \"their is a good weather\".split(),\n","            \"there my friends\".split(),\n","            ]"]},{"cell_type":"markdown","metadata":{"id":"MN-iS76XpWWd"},"source":["For the spell checker,"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"UtXWSTe5pWWd","executionInfo":{"status":"ok","timestamp":1642260443583,"user_tz":-60,"elapsed":275,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"outputs":[],"source":["def spell_check(sent, common_errors):\n","    sents = []\n","    probs = []\n","    sents.append(sent)\n","    probs.append(sentence_logP(sent))\n","    \n","    # create new hypothesis sentences by recursively applying all possible spelling mistakes to \n","    # each token in the sentence. If the new sentence is not the same as the original, append\n","    # it to sents and compute its probability and append it to probs.\n","    for i, token in enumerate(sent):\n","        for error, correct in common_errors.items():\n","            if token.find(error) != -1:\n","                new_token = token.replace(error, correct, 1)\n","                new_sent = sent.copy()\n","                new_sent[i] = new_token\n","                sents.append(new_sent)\n","                probs.append(sentence_logP(new_sent))\n","        \n","    # Finally take the argmax of the probabilities and return that sentence\n","    # max_i = np.argmin(probs)\n","    # return sents[max_i], probs[max_i]\n","    min_i = np.argmin(probs)\n","    return sents[min_i], probs[min_i]"]},{"cell_type":"markdown","metadata":{"id":"PdNOa6iEpWWe"},"source":["It would be a good idea to retrain your langauge model on all of the Brown sentences (brown.sents()) in order to improve it's recall.\n","\n","1. After retraining, do you notice any differences?"]},{"cell_type":"code","source":["train = brown.sents()\n","counts = estimate_lm(train)"],"metadata":{"id":"eOlcTmjsemjU","executionInfo":{"status":"ok","timestamp":1642260563588,"user_tz":-60,"elapsed":7972,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["for sent in test_set:\n","    sent_check, prob = spell_check(sent, common_errors)\n","    print(\"Original:\", \" \".join(sent), sentence_logP(sent))\n","    print(\"Correct:\", \" \".join(sent_check), prob)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XfroVBx6fUjU","executionInfo":{"status":"ok","timestamp":1642260569079,"user_tz":-60,"elapsed":283,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"0e25541e-49e1-49b6-e7b0-bf9110f4e339"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["Original: I do not know who acheived it -23.962760536913688\n","Correct: I do not know who achieved it -31.564662496788856\n","Original: I do not know who recieved it -23.963159977701903\n","Correct: I do not know who received it -32.48155244969362\n","Original: That is not apparant -23.60046156453007\n","Correct: That is not apparent -23.60046156453007\n","Original: We can only accomodate one person -38.29513646861539\n","Correct: We can only accommodate one person -37.60198928805545\n","Original: That is not profesional -23.600465386977394\n","Correct: That is not professional -30.510218668622205\n","Original: we saw teh man running -26.853802458286175\n","Correct: we saw the man running -35.17095969143966\n","Original: We tried too help them -34.57383974386029\n","Correct: We tried to help them -42.40575643176704\n","Original: their is a good weather -41.53606320227949\n","Correct: their is a good weather -41.53597994515282\n","Original: there my friends -27.664014374023065\n","Correct: they're my friends -26.566130361233142\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"Language Model Exercises.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}